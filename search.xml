<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[ML基础]]></title>
      <url>%2F2018%2F06%2F12%2FML%E5%9F%BA%E7%A1%80%2F</url>
      <content type="text"><![CDATA[前言本文主要梳理Machine Learning中的必要基础。 概率论与数理统计1. 先验概率‘先验’一词，在拉丁语中表示’来自先前的东西’，或者‘在经验之前’。先验概率，指随机事件发生前的预判概率。它可以是基于历史数据的统计，也可以由背景常识得出，或者是人的主观观点给出。它往往作为‘由因求果’问题中的‘因’出现，比如考察‘瓜熟蒂落’这个因果事件，‘瓜熟’为‘因’，故而其概率为先验概率，可根据统计、常识或主观判断给出。2. 后验概率就是指在知道‘果’之后，去推测‘因’的概率，是‘执果寻因’问题中的‘因’。在‘瓜熟蒂落’的例子中，如果已经知道瓜蒂脱落，那么瓜熟的概率就是后验概率。后验概率与先验概率之间可以通过贝叶斯公式联系，也就是：$$P(瓜熟|蒂落)=\frac{P(瓜熟)\times P(蒂落|瓜熟)}{P(蒂落)}$$ 极大似然估计 卷积 贝叶斯定理 熵 条件熵 信息熵 互信息 条件互信息 信息增益 基尼指数]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[高斯混合模型（GMM）]]></title>
      <url>%2F2017%2F05%2F15%2F%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%2F</url>
      <content type="text"><![CDATA[前言GMM是一种提出于上世纪60年代、并在其后被广泛研究而日渐完善的一种分类与回归分析模型，以其模型简单、分类能力强等优势而被广泛地应用于各类问题中。本文就高斯混合模型（GMM,Gaussian Mixture Model）参数如何确立这个问题，详细讲解期望最大化（EM,Expectation Maximization）算法的实施过程。 起式SVM，中文名为支持向量机或支持向量网络，简单来说，它的基本形式是一种二类线性监督式分类或回归学习模型。通过在样本空间中寻找符合某种最优条件的超平面，将带有类别标签的训练数据进行分类，从而得到分类准则，运用于测试样本集，判断测试样本的所属类别。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[SVM：原理与应用]]></title>
      <url>%2F2016%2F12%2F10%2FSVM-Tutorial%2F</url>
      <content type="text"><![CDATA[前言SVM是一种提出于上世纪60年代、并在其后被广泛研究而日渐完善的一种分类与回归分析模型，以其模型简单、分类能力强等优势而被广泛地应用于各类问题中。关于SVM的著述和文章可谓汗牛充栋，有专业研究者的论文，也有面向一般读者的科普性文章；前者注重算法研究，涉及大量公式推导与证明；后者注重原理介绍，尽量以浅显易懂的语言将基本原理介绍清楚；前者保证了数学的准确性，后者提供了数学意义的直观认识。作为一名非机器学习领域的科研者，我想尝试着将二者进行结合，因为我认为：任何高深复杂的东西都可以用尽可能准确又直观的讲解表达出来，如果做不到，那一定是讲者自身的理解不通透。本文将着重从SVM的原理入手，辅之以应用的介绍，以期能够帮助想要理解和运用SVM的人尽快入门。然而，需要指出的是：受限于个人水平的问题，文中的错误在所难免，甚至可能有很多。所以，还望大家海涵，同时提出您的批评和指正意见。本文在写作过程中参考了很多文章，有研究论文，也有博客文章等，在本文的末尾将会以链接形式给出。在此一并谢过！以下是博客原文，请阅 历史 “这要从1989年说起，我那时正在研究神经网络和核方法的性能对比，直到我的丈夫决定使用Vladimir的算法，SVM就诞生了。” -by Isabelle Guyon 1963年，Vladimir Vapnik在解决模式识别问题时提出了支持向量方法； 1971年，Kimeldorf构造基于支持向量构建核空间的方法； Vapnik等人正式提出统计学习理论。 早在20世纪60年代，Vapnik就已奠定了统计学习的基本理论基础，如经验风险最小化原则下统计学习一致性的条件（收敛性、收敛的可控性、收敛与概率测度定义的无关性，号称机器学习理论的“三个里程碑”）、关于统计学习方法推广性的界的理论，以及在此基础上建立的小样本归纳推理原则等。直到20世纪90年代中后期，能够实现统计学习理论和原则的实用性算法——SVM方法才逐渐被完整地提出，并且在模式识别等人工智能领域得到成功应用，受到广泛地关注。 起式SVM，中文名为支持向量机或支持向量网络，简单来说，它的基本形式是一种二类线性监督式分类或回归学习模型。通过在样本空间中寻找符合某种最优条件的超平面，将带有类别标签的训练数据进行分类，从而得到分类准则，运用于测试样本集，判断测试样本的所属类别。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PCA原理详解]]></title>
      <url>%2F2016%2F12%2F09%2FPCA%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%2F</url>
      <content type="text"><![CDATA[最近看了一些关于降维算法的东西，本文首先给出了七种算法的一个信息表，归纳了关于每个算法可以调节的(超)参数、算法主要目的等等，然后介绍了降维的一些基本概念，包括降维是什么、为什么要降维、降维可以解决维数灾难等，然后分析可以从什么样的角度来降维，接着整理了这些算法的具体流程。 而为什么可以降维，这是因为数据有冗余，要么是一些没有用的信息，要么是一些重复表达的信息，例如一张512512的图只有中心100100的区域内有非0值，剩下的区域就是没有用的信息，又或者一张图是成中心对称的，那么对称的部分信息就重复了。正确降维后的数据一般保留了原始数据的大部分的重要信息，它完全可以替代输入去做一些其他的工作，从而很大程度上可以减少计算量。例如降到二维或者三维来可视化。 而为什么可以降维，这是因为数据有冗余，要么是一些没有用的信息，要么是一些重复表达的信息，例如一张512512的图只有中心100100的区域内有非0值，剩下的区域就是没有用的信息，又或者一张图是成中心对称的，那么对称的部分信息就重复了。正确降维后的数据一般保留了原始数据的大部分的重要信息，它完全可以替代输入去做一些其他的工作，从而很大程度上可以减少计算量。例如降到二维或者三维来可视化。 而为什么可以降维，这是因为数据有冗余，要么是一些没有用的信息，要么是一些重复表达的信息，例如一张512512的图只有中心100100的区域内有非0值，剩下的区域就是没有用的信息，又或者一张图是成中心对称的，那么对称的部分信息就重复了。正确降维后的数据一般保留了原始数据的大部分的重要信息，它完全可以替代输入去做一些其他的工作，从而很大程度上可以减少计算量。例如降到二维或者三维来可视化。 而为什么可以降维，这是因为数据有冗余，要么是一些没有用的信息，要么是一些重复表达的信息，例如一张512512的图只有中心100100的区域内有非0值，剩下的区域就是没有用的信息，又或者一张图是成中心对称的，那么对称的部分信息就重复了。正确降维后的数据一般保留了原始数据的大部分的重要信息，它完全可以替代输入去做一些其他的工作，从而很大程度上可以减少计算量。例如降到二维或者三维来可视化。 而为什么可以降维，这是因为数据有冗余，要么是一些没有用的信息，要么是一些重复表达的信息，例如一张512512的图只有中心100100的区域内有非0值，剩下的区域就是没有用的信息，又或者一张图是成中心对称的，那么对称的部分信息就重复了。正确降维后的数据一般保留了原始数据的大部分的重要信息，它完全可以替代输入去做一些其他的工作，从而很大程度上可以减少计算量。例如降到二维或者三维来可视化。 $c=a+b$这是因为数据有冗余，要么是一些没有用的信息，要么是一些重复表达的信息，$c=a^2$，另一个公式可以表达为$E=m\cdot c^2$ 被誉为最美的数学公式:$e^{\pi i} + 1 = 0$，即为传说中的欧拉公式。上面是在电脑A上做的工作,接下来回到电脑B. 首先, 像在电脑A上搭建博客一样, 要在B上安装git, node.js, hexo 安装过程自己Google(既然在A上搭好博客了,安装过程应该会了). 然后把github上的repository克隆到电脑B上。首先, 在github上再键一个repository, 假定为HexoBlog. 进到电脑A上的博客的文件夹. 如果你的博客主题是从github上克隆下来的, 要把主题文件夹里的.git文件夹删掉. 以我自己的为例, 我用的主题是yilia, 用 ls -la命令就看到 thems/yilia/ 下面的隐藏文件夹.git. 然后删掉。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Gradient]]></title>
      <url>%2F2016%2F12%2F08%2FGradient%2F</url>
      <content type="text"><![CDATA[本文是David Silver强化学习公开课第六课的总结笔记。这一课主要讲了由于现实问题中状态数过多导致无法直接求解出值函数，从而通过梯度下降的方式来求解真实值函数的近似函数形式。Simple inline $a = b + c$. 前两节课我忽略了一些内容，这节课用到了，所以先回顾一下。首先是Planning的概念，在第一节课提到过强化学习是一种Sequential Decision Making问题，它是一种试错(trial-and-error)的学习方式，一开始不清楚environment的工作方式，不清楚执行什么样的行为是对的，什么样是错的，因而agent需要从不断尝试的经验中发现一个好的policy。而Planning也属于Sequential Decision Making问题，不同的是它的environment是已知的，例如游戏的规则是已知的，所以agent不需要通过与environment的交互来获取下一个状态，而是知道自己执行某个action之后状态是什么，再优化自己的policy。因此这两者之间是有联系的，假如强化学习学习出来environment的模型，知道了environment是如何work的，强化学习要解决的问题就是Planning了。 前两节课我忽略了一些内容，这节课用到了，所以先回顾一下。首先是Planning的概念，在第一节课提到过强化学习是一种Sequential Decision Making问题，它是一种试错(trial-and-error)的学习方式，一开始不清楚environment的工作方式，不清楚执行什么样的行为是对的，什么样是错的，因而agent需要从不断尝试的经验中发现一个好的policy。而Planning也属于Sequential Decision Making问题，不同的是它的environment是已知的，例如游戏的规则是已知的，所以agent不需要通过与environment的交互来获取下一个状态，而是知道自己执行某个action之后状态是什么，再优化自己的policy。因此这两者之间是有联系的，假如强化学习学习出来environment的模型，知道了environment是如何work的，强化学习要解决的问题就是Planning了。 前两节课我忽略了一些内容，这节课用到了，所以先回顾一下。首先是Planning的概念，在第一节课提到过强化学习是一种Sequential Decision Making问题，它是一种试错(trial-and-error)的学习方式，一开始不清楚environment的工作方式，不清楚执行什么样的行为是对的，什么样是错的，因而agent需要从不断尝试的经验中发现一个好的policy。而Planning也属于Sequential Decision Making问题，不同的是它的environment是已知的，例如游戏的规则是已知的，所以agent不需要通过与environment的交互来获取下一个状态，而是知道自己执行某个action之后状态是什么，再优化自己的policy。因此这两者之间是有联系的，假如强化学习学习出来environment的模型，知道了environment是如何work的，强化学习要解决的问题就是Planning了。 前两节课我忽略了一些内容，这节课用到了，所以先回顾一下。首先是Planning的概念，在第一节课提到过强化学习是一种Sequential Decision Making问题，它是一种试错(trial-and-error)的学习方式，一开始不清楚environment的工作方式，不清楚执行什么样的行为是对的，什么样是错的，因而agent需要从不断尝试的经验中发现一个好的policy。而Planning也属于Sequential Decision Making问题，不同的是它的environment是已知的，例如游戏的规则是已知的，所以agent不需要通过与environment的交互来获取下一个状态，而是知道自己执行某个action之后状态是什么，再优化自己的policy。因此这两者之间是有联系的，假如强化学习学习出来environment的模型，知道了environment是如何work的，强化学习要解决的问题就是Planning了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Gradient实现CartPole]]></title>
      <url>%2F2016%2F12%2F08%2FGradient%E5%AE%9E%E7%8E%B0CartPole%2F</url>
      <content type="text"><![CDATA[8月的时候把David silver的强化学习课上了，但是一直对其中概念如何映射到现实问题中不理解，半个月前突然发现OpenAI提供了一个python库Gym，它创造了强化学习的environment，可以很方便的启动一个强化学习任务来自己实现算法，并且提供了不少可以解决的问题来练手。本文针对如何解决入门问题CartPole，来解释一下怎么将之前课上的算法转化成实现代码。 CartPole的玩法如下动图所示，目标就是保持一根杆一直竖直朝上，杆由于重力原因会一直倾斜，当杆倾斜到一定程度就会倒下，此时需要朝左或者右移动杆保证它不会倒下来。我们执行一个动作，动作取值为0或1，代表向左或向右移动，返回的observation是一个四维向量，reward值一直是1，当杆倒下时done的取值为False，其他为True，info是调试信息打印为空具体使用暂时不清楚。如果杆竖直向上的时间越长，得到reward的次数就越多。 由于policy中权重也是一个四维向量，如果随机给四维向量赋值，有机会得到比较好的policy。首先先实现一个函数用来衡量给定的某组权重效果如何，函数返回值是这组权重下得到的奖赏，意义是杆维持了多长时间未倒下，代码如下。由于policy中权重也是一个四维向量，如果随机给四维向量赋值，有机会得到比较好的policy。首先先实现一个函数用来衡量给定的某组权重效果如何，函数返回值是这组权重下得到的奖赏，意义是杆维持了多长时间未倒下，代码如下。由于policy中权重也是一个四维向量，如果随机给四维向量赋值，有机会得到比较好的policy。首先先实现一个函数用来衡量给定的某组权重效果如何，函数返回值是这组权重下得到的奖赏，意义是杆维持了多长时间未倒下，代码如下。由于policy中权重也是一个四维向量，如果随机给四维向量赋值，有机会得到比较好的policy。首先先实现一个函数用来衡量给定的某组权重效果如何，函数返回值是这组权重下得到的奖赏，意义是杆维持了多长时间未倒下，代码如下。由于policy中权重也是一个四维向量，如果随机给四维向量赋值，有机会得到比较好的policy。首先先实现一个函数用来衡量给定的某组权重效果如何，函数返回值是这组权重下得到的奖赏，意义是杆维持了多长时间未倒下，代码如下。由于policy中权重也是一个四维向量，如果随机给四维向量赋值，有机会得到比较好的policy。首先先实现一个函数用来衡量给定的某组权重效果如何，函数返回值是这组权重下得到的奖赏，意义是杆维持了多长时间未倒下，代码如下。由于policy中权重也是一个四维向量，如果随机给四维向量赋值，有机会得到比较好的policy。首先先实现一个函数用来衡量给定的某组权重效果如何，函数返回值是这组权重下得到的奖赏，意义是杆维持了多长时间未倒下，代码如下。由于policy中权重也是一个四维向量，如果随机给四维向量赋值，有机会得到比较好的policy。首先先实现一个函数用来衡量给定的某组权重效果如何，函数返回值是这组权重下得到的奖赏，意义是杆维持了多长时间未倒下，代码如下。由于policy中权重也是一个四维向量，如果随机给四维向量赋值，有机会得到比较好的policy。首先先实现一个函数用来衡量给定的某组权重效果如何，函数返回值是这组权重下得到的奖赏，意义是杆维持了多长时间未倒下，代码如下。由于policy中权重也是一个四维向量，如果随机给四维向量赋值，有机会得到比较好的policy。首先先实现一个函数用来衡量给定的某组权重效果如何，函数返回值是这组权重下得到的奖赏，意义是杆维持了多长时间未倒下，代码如下。由于policy中权重也是一个四维向量，如果随机给四维向量赋值，有机会得到比较好的policy。首先先实现一个函数用来衡量给定的某组权重效果如何，函数返回值是这组权重下得到的奖赏，意义是杆维持了多长时间未倒下，代码如下。由于policy中权重也是一个四维向量，如果随机给四维向量赋值，有机会得到比较好的policy。首先先实现一个函数用来衡量给定的某组权重效果如何，函数返回值是这组权重下得到的奖赏，意义是杆维持了多长时间未倒下，代码如下。由于policy中权重也是一个四维向量，如果随机给四维向量赋值，有机会得到比较好的policy。首先先实现一个函数用来衡量给定的某组权重效果如何，函数返回值是这组权重下得到的奖赏，意义是杆维持了多长时间未倒下，代码如下。由于policy中权重也是一个四维向量，如果随机给四维向量赋值，有机会得到比较好的policy。首先先实现一个函数用来衡量给定的某组权重效果如何，函数返回值是这组权重下得到的奖赏，意义是杆维持了多长时间未倒下，代码如下。由于policy中权重也是一个四维向量，如果随机给四维向量赋值，有机会得到比较好的policy。首先先实现一个函数用来衡量给定的某组权重效果如何，函数返回值是这组权重下得到的奖赏，意义是杆维持了多长时间未倒下，代码如下。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[HelloWorld]]></title>
      <url>%2F2016%2F12%2F08%2FHelloWorld%2F</url>
      <content type="text"><![CDATA[#This is HelloWorld8月的时候把David Silver的强化学习课上了，但是一直对其中概念如何映射到现实问题中不理解，半个月前突然发现OpenAI提供了一个python库Gym，它创造了强化学习的environment，可以很方便的启动一个强化学习任务来自己实现算法，并且提供了不少可以解决的问题来练手。本文针对如何解决入门问题CartPole，来解释一下怎么将之前课上的算法转化成实现代码。本文主要阐述了对生成式对抗网络的理解，首先谈到了什么是对抗样本，以及它与对抗网络的关系，然后解释了对抗网络的每个组成部分，再结合算法流程和代码实现来解释具体是如何实现并执行这个算法的，最后通过给出一个基于对抗网络改写的去噪网络，效果虽然挺差的，但是还是挺有意思的。 本文主要阐述了对生成式对抗网络的理解，首先谈到了什么是对抗样本，以及它与对抗网络的关系，然后解释了对抗网络的每个组成部分，再结合算法流程和代码实现来解释具体是如何实现并执行这个算法的，最后给出一个基于对抗网络改写的去噪网络运行的结果，效果虽然挺差的，但是有些地方还是挺有意思的。 14年的时候Szegedy在研究神经网络的性质时，发现针对一个已经训练好的分类模型，将训练集中样本做一些细微的改变会导致模型给出一个错误的分类结果，这种虽然发生扰动但是人眼可能识别不出来，并且会导致误分类的样本被称为对抗样本，他们利用这样的样本发明了对抗训练(adversarial training)，模型既训练正常的样本也训练这种自己造的对抗样本，从而改进模型的泛化能力[1]。如下图所示，在未加扰动之前，模型认为输入图片有57.7%的概率为熊猫，但是加了之后，人眼看着好像没有发生改变，但是模型却认为有99.3%的可能是长臂猿。 这个问题乍一看很像过拟合，在Goodfellow在15年[3]提到了其实模型欠拟合也能导致对抗样本，因为从现象上来说是输入发生了一定程度的改变就导致了输出的不正确，例如下图一，上下分别是过拟合和欠拟合导致的对抗样本，其中绿色的o和x代表训练集，红色的o和x即对抗样本，明显可以看到欠拟合的情况下输入发生改变也会导致分类不正确(其实这里我觉得有点奇怪，因为图中所描述的对抗样本不一定就是跟原始样本是同分布的，感觉是人为造的一个东西，而不是真实数据的反馈)。在[1]中作者觉得这种现象可能是因为神经网络的非线性和过拟合导致的，但Goodfellow却给出了更为准确的解释，即对抗样本误分类是因为模型的线性性质导致的，说白了就是因为]]></content>
    </entry>

    
  
  
</search>
